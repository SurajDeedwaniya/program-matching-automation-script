# -*- coding: utf-8 -*-
"""final-program-data-sync-agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sCJJf1zOsWK6nKOjmP94dBZ2XKsL0bfD
"""

# def install_dependencies():
#   !pip install --upgrade gspread oauth2client
#   !pip install tabulate
#   !pip install -q -U google-generativeai
#   !pip install pymupdf --quiet
#   !pip install requests --quiet
#   !apt-get update
#   !apt-get install -y tesseract-ocr
#   !pip install pytesseract
#   !apt-get install -y libtesseract-dev

install_dependencies()

import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import re
import json
import logging
import sys
import difflib
import unicodedata
from tabulate import tabulate
from datetime import datetime
import requests
import json
import re
import logging
import sys
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from tabulate import tabulate
from difflib import SequenceMatcher

client = None
spreadsheet_url = None

# ‚úÖ Configure logging
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

log_file_path = '/content/mismatch_debug.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler(sys.stdout)
    ]
)

def load_sheet_data():
    global client, spreadsheet_url  # ‚úÖ Ensure globals are referenced properly

    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    credentials_path = '/content/service_account.json'
    creds = ServiceAccountCredentials.from_json_keyfile_name(credentials_path, scope)

    client = gspread.authorize(creds)
    spreadsheet_url = "https://docs.google.com/spreadsheets/d/13s0yN_9fhspaezqqjfZP1OuziIChJM2aD3kmkjP4mhU/edit?gid=0"

    sheet = client.open_by_url(spreadsheet_url).sheet1
    data = sheet.get_all_records()
    df = pd.DataFrame(data)

    print("üîç Previewing Google Sheet Data:")
    try:
        from IPython.display import display
        display(df.head())
    except Exception:
        # In CI there is no IPython display ‚Äî fallback to printing head
        print(df.head().to_string(index=False))

def extract_first_number(text):
    """Extracts the first integer number from text."""
    nums = re.findall(r'\d+', text)
    return int(nums[0]) if nums else None

field_aliases = {
    "class hours": "hours",
    "total class hours": "hours",
    # "tools and frameworks": "tools",
    "career assistance": "career sessions",
    "modules": "modules",
    # "pillars/terms": "pillars",
    # "pre/post reads, assignments, quizzes": "assignments",
    "campus immersion": "campus immersion"
}


def normalize_tool_name(name):
          """Normalize tool names: remove punctuation, normalize unicode, lowercase."""
          name = unicodedata.normalize('NFKD', name)
          name = re.sub(r'[^\w\s]', '', name)  # Remove punctuation
          return name.strip().lower()


def run_mismatch_checker():
    global client, spreadsheet_url
    logging.info("üöÄ Starting mismatch checker...")
    print("üöÄ Starting mismatch checker...")
    sheet_names = [ws.title for ws in client.open_by_url(spreadsheet_url).worksheets()]
    logging.info("üìò All sheet tabs: %s", sheet_names)
    print("üìò All sheet tabs:", sheet_names)

    logging.info("üìÇ Loading JSON from programs.json...")
    with open('/content/programs.json', 'r') as f:
        json_data = json.load(f)

    json_programs_by_id = {str(p['id']).strip().lower(): p for p in json_data}
    logging.info("‚úÖ Loaded %d JSON programs", len(json_programs_by_id))

    all_mismatches = []
    all_matches = []
    all_traversed_ids = []
    for sheet_name in sheet_names:
        print(f"\nüîç Processing sheet: {sheet_name}")
        logging.info("üîç Processing sheet: %s", sheet_name)

        worksheet = client.open_by_url(spreadsheet_url).worksheet(sheet_name)
        df = pd.DataFrame(worksheet.get_all_records())

        if 'id' not in df.columns:
            logging.warning("‚ö†Ô∏è Sheet '%s' skipped: Missing 'id' column", sheet_name)
            print(f"‚ö†Ô∏è Sheet '{sheet_name}' skipped: Missing 'id' column.")
            continue

        df_multi = df.set_index('id')

        for program_id in df_multi.columns:

            sheet_id_clean = str(program_id).strip().lower()
            program_name_from_sheet = df_multi[program_id].get('Program Name', '').strip()

            all_traversed_ids.append({
                "id": program_id,
                "sheet": sheet_name,
                "program_name": program_name_from_sheet
            })
            # all_traversed_ids.append({
            #    "id": program_id,
            #    "sheet": sheet_name
            # })

            if sheet_id_clean not in json_programs_by_id:
                logging.warning("‚ùó MISMATCH | Sheet: %s | ID: %s | Issue: Program ID not found in JSON", sheet_name, sheet_id_clean)
                all_mismatches.append({
                    "id": program_id,
                    "sheet": sheet_name,
                    "issue": "Program ID not found in programs.json"
                })
                continue

            json_program = json_programs_by_id[sheet_id_clean]
            sheet_kv = df_multi[program_id].fillna("").astype(str).to_dict()

            simple_mapping = {
                'Program Name': 'program_name',
                'Duration': 'duration',
                'Preferred Years of Experience (As per Brochure)': 'required_years_experience',
                'Retail Price': 'total'
            }

            for sheet_key, json_key in simple_mapping.items():
                if json_key == 'required_years_experience':
                    experience_key = (
                        'Preferred Years of Experience (As per Brochure)' if sheet_name == 'IIMV'
                        else 'Total Years of Minimum Experience'
                    )
                    sheet_key = experience_key

                sheet_value = sheet_kv.get(sheet_key, '').strip().lower()
                json_value = str(json_program.get(json_key, '')).strip().lower()

                if sheet_key == 'Program Name':
                    sheet_value = re.sub(r'by.*$', '', sheet_value)
                    sheet_value = re.sub(r'- batch.*$', '', sheet_value).strip()
                    match = re.search(r'\d+', sheet_value)
                    if match:
                        sheet_value = match.group()

                if sheet_key == 'Retail Price':
                    sheet_value = re.sub(r'[^\d]', '', sheet_value)

                if sheet_value and sheet_value != json_value:
                    logging.info("‚ùå MISMATCH | Sheet: %s | ID: %s | Field: %s | Sheet Value: '%s' | JSON Value: '%s'",
                                 sheet_name, program_id, sheet_key, sheet_value, json_value)
                    all_mismatches.append({
                        "id": program_id,
                        "program_name": json_program.get("program_name", ""),
                        "sheet": sheet_name,
                        "field": sheet_key,
                        "sheet_value": sheet_value,
                        "json_value": json_value
                    })

            start_raw = sheet_kv.get('Batch Start Date', '').strip()
            json_date = str(json_program.get('start_date', '')).strip().lower()

            if start_raw:
                logging.info("üìÖ Raw start date field: '%s'", start_raw)
                match = re.search(r'(\d{1,2})(st|nd|rd|th)?\s+(\w+)\s+(\d{4})', start_raw, re.IGNORECASE)
                if match:
                    day = match.group(1)
                    month = match.group(3)
                    year = match.group(4)
                    sheet_date = f"{day} {month} {year}".strip().lower()
                else:
                    sheet_date = "UNPARSEABLE"
                    logging.warning("‚ö†Ô∏è Could not parse date from 'Batch Start Date' in sheet: %s, ID: %s", sheet_name, program_id)

                logging.info("üìÖ Final Parsed Sheet Start Date: '%s' | JSON Start Date: '%s'", sheet_date, json_date)

                if sheet_date != "UNPARSEABLE" and sheet_date != json_date:
                    logging.info("‚ùå MISMATCH | Sheet: %s | ID: %s | Field: start_date | Sheet Value: '%s' | JSON Value: '%s'",
                                 sheet_name, program_id, sheet_date, json_date)
                    all_mismatches.append({
                        "id": program_id,
                        "program_name": json_program.get("program_name", ""),
                        "sheet": sheet_name,
                        "field": "start_date",
                        "sheet_value": sheet_date,
                        "json_value": json_date
                    })
            else:
                logging.warning("‚ö†Ô∏è No 'Batch Start Date' found in sheet: %s, ID: %s", sheet_name, program_id)

            normalized_sheet_kv = {k.lower().strip(): v for k, v in sheet_kv.items()}

            for months in [12, 18, 24]:
                label = f"{months} months emi"
                sheet_value_raw = normalized_sheet_kv.get(label, '').replace(',', '').strip()

                if not sheet_value_raw:
                    logging.warning("‚ö†Ô∏è Skipped EMI check | Sheet: %s | ID: %s | EMI Term: %s | Reason: Empty or missing value",
                                    sheet_name, program_id, label)
                    continue

                try:
                    sheet_emi = float(sheet_value_raw)
                    expected = next((emi['emi'] for emi in json_program.get('monthly_emi', []) if emi['months'] == months), None)

                    if expected is not None:
                        if abs(sheet_emi - expected) > 0.01:
                            logging.info("‚ùå MISMATCH | Sheet: %s | ID: %s | Field: %s | Sheet Value: '%s' | JSON Value: '%s'",
                                         sheet_name, program_id, label, sheet_value_raw, expected)
                            all_mismatches.append({
                                "id": program_id,
                                "program_name": json_program.get("program_name", ""),
                                "sheet": sheet_name,
                                "field": label,
                                "sheet_value": sheet_value_raw,
                                "json_value": expected
                            })
                        else:
                            logging.info("‚úÖ MATCH | Sheet: %s | ID: %s | EMI Term: %s | Sheet Value: '%s' | JSON Value: '%s'",
                                         sheet_name, program_id, label, sheet_value_raw, expected)
                except ValueError:
                    logging.warning("‚ö†Ô∏è Skipped EMI check | Sheet: %s | ID: %s | EMI Term: %s | Reason: Non-numeric value '%s'",
                                    sheet_name, program_id, label, sheet_value_raw)

            # tools_sheet_raw = sheet_kv.get("Tools and Frameworks", "").strip()

            # if not tools_sheet_raw or tools_sheet_raw.lower() in ["n.a", "na", ""]:
            #     logging.info("‚ö™ Skipping tools comparison due to empty or 'NA' value in sheet: '%s'", tools_sheet_raw)
            # else:
            #     tools_json = [normalize_tool_name(t['title']) for t in json_program.get('tools', [])]
            #     raw_items = re.split(r'[\n,;]+', tools_sheet_raw)
            #     tools_list = []

            #     for item in raw_items:
            #         original_item = item.strip()
            #         cleaned = re.sub(r'^(10\+)?\s*(pm|dt)?\s*tools:?', '', original_item, flags=re.IGNORECASE).strip()
            #         if cleaned:
            #             normalized = normalize_tool_name(cleaned)
            #             tools_list.append(normalized)

            #     logging.info("üîß Parsed tools from sheet: %s", tools_list)
            #     logging.info("üéØ Expected tools from JSON: %s", tools_json)

            #     for tool in tools_list:
            #         if tool not in tools_json:
            #             logging.info(
            #                 "‚ùå MISMATCH | Sheet: %s | ID: %s | Field: tools | Sheet Tool: '%s' | Not found in JSON tool list.",
            #                 sheet_name, program_id, tool
            #             )
            #             all_mismatches.append({
            #                 "id": program_id,
            #                 "sheet": sheet_name,
            #                 "field": "tools",
            #                 "sheet_value": tool,
            #                 "json_value": tools_json
            #             })

            tools_sheet_raw = sheet_kv.get("Tools and Frameworks", "").strip()

            if not tools_sheet_raw or tools_sheet_raw.lower() in ["n.a", "na", ""]:
                logging.info("‚ö™ Skipping tools comparison due to empty or 'NA' value in sheet: '%s'", tools_sheet_raw)
            else:
                tools_json = [normalize_tool_name(t['title']) for t in json_program.get('tools', [])]
                raw_items = re.split(r'[\n,;]+', tools_sheet_raw)
                tools_list = []

                for item in raw_items:
                    original_item = item.strip()

                    # Remove numeric prefixes like "10+ Tools:", "15 Tools:", etc.
                    cleaned = re.sub(r'^\d+\+?\s*tools?:?', '', original_item, flags=re.IGNORECASE).strip()

                    # Remove section labels like "AI for Leaders:", "Project Management:", etc.
                    cleaned = re.sub(r'^[a-z\s]*:?$', '', cleaned, flags=re.IGNORECASE).strip()

                    if not cleaned:
                        continue

                    if ':' in cleaned:
                        # Example: "AI for Leaders: Python"
                        parts = cleaned.split(':', 1)
                        cleaned = parts[1].strip()

                    normalized = normalize_tool_name(cleaned)
                    if normalized in tools_json:
                        tools_list.append(normalized)
                    else:
                        logging.info(
                            "‚ùå MISMATCH | Sheet: %s | ID: %s | Field: tools | Sheet Tool: '%s' | Not found in JSON tool list.",
                            sheet_name, program_id, normalized
                        )
                        all_mismatches.append({
                            "id": program_id,
                            "program_name": json_program.get("program_name", ""),
                            "sheet": sheet_name,
                            "field": "tools",
                            "sheet_value": cleaned,
                            "json_value": tools_json
                        })

                logging.info("üîß Parsed tools from sheet: %s", tools_list)
                logging.info("üéØ Expected tools from JSON: %s", tools_json)



                existing_mismatches = [
                    m for m in all_mismatches if m['id'] == program_id and m['sheet'] == sheet_name
                ]
                if not existing_mismatches:
                    all_matches.append({
                        "id": program_id,
                        "program_name": json_program.get("program_name", ""),
                        "sheet": sheet_name
                    })




            # for key in sheet_kv:
            #     lower = key.lower()
            #     feature_value = sheet_kv[key].strip().lower()
            #     matched = False

            #     if any(term in lower for term in ['total class hours', 'hour', 'career', 'assignment', 'modules',
            #                                       'campus immersion', 'career assistance', 'tools', 'pillar', 'term']):
            #         closest_title = None
            #         closest_ratio = 0
            #         closest_feat_val = ""

            #         for feat in json_program.get("key_features", []):
            #             feat_title = feat.get("title", "").lower()
            #             feat_val_raw = feat.get("count") if feat.get("count") is not None else feat.get("text", "")
            #             feat_val = str(feat_val_raw).strip().lower()

            #             norm_feat_title = re.sub(r'[^a-z0-9]', '', feat_title)
            #             norm_key = re.sub(r'[^a-z0-9]', '', lower)

            #             similarity_ratio = difflib.SequenceMatcher(None, norm_feat_title, norm_key).ratio()
            #             if similarity_ratio > closest_ratio:
            #                 closest_ratio = similarity_ratio
            #                 closest_title = feat_title
            #                 closest_feat_val = feat_val

            #             digits_in_sheet = re.findall(r'\d+', feature_value)
            #             digits_in_json = re.findall(r'\d+', feat_val)
            #             numeric_match = False
            #             if digits_in_sheet and digits_in_json:
            #                 try:
            #                     num_sheet = int(digits_in_sheet[0])
            #                     num_json = int(digits_in_json[0])
            #                     if abs(num_sheet - num_json) <= max(1, int(0.1 * num_json)):
            #                         numeric_match = True
            #                 except:
            #                     pass

            #             exact_match = feat_val == feature_value or feat_val in feature_value or feature_value in feat_val

            #             if similarity_ratio >= 0.75 and (exact_match or numeric_match):
            #                 matched = True
            #                 break

            #         if not matched and feature_value not in ["", "n.a", "na"]:
            #             logging.info(
            #                 "‚ùå MISMATCH | Sheet: %s | ID: %s | Field: %s | Sheet Value: '%s' | Closest Title: '%s' (%.2f) | JSON Value: '%s'",
            #                 sheet_name,
            #                 program_id,
            #                 key,
            #                 feature_value,
            #                 closest_title if closest_title else "None",
            #                 closest_ratio,
            #                 closest_feat_val if closest_feat_val else "N/A"
            #             )
            #             all_mismatches.append({
            #                 "id": program_id,
            #                 "sheet": sheet_name,
            #                 "field": key,
            #                 "sheet_value": feature_value,
            #                 "json_value": "Not matched in key_features"
            #             })

    logging.info("üìù Writing mismatch report with %d items", len(all_mismatches))
    with open('/content/mismatch_report.json', 'w') as f:
        json.dump(all_mismatches, f, indent=2)


    logging.info("üìù Writing match report with %d items", len(all_matches))
    with open('/content/match_report.json', 'w') as f:
        json.dump(all_matches, f, indent=2)

    with open('/content/all_traversed_ids.json', 'w') as f:
        json.dump(all_traversed_ids, f, indent=2)

    logging.info("üìÅ Saved all traversed program IDs to: all_traversed_ids.json")

    print(f"\nüìÜ Finished scanning all sheets.")
    print(f"‚ùå Total mismatches found: {len(all_mismatches)}")
    logging.info("‚úÖ Process complete. Mismatches found: %d", len(all_mismatches))

    if not all_mismatches:
        print("‚úÖ No mismatches to show.")
    else:
        print("\nüîç Mismatch Details:\n")
        table_data = []
        for m in all_mismatches:
            table_data.append([
                m.get("sheet", ""),
                m.get("id", ""),
                m.get("field", m.get("issue", "")),
                m.get("sheet_value", ""),
                m.get("json_value", "")
            ])
        headers = ["Sheet", "Program ID", "Field", "Sheet Value", "JSON Value"]
        print(tabulate(table_data, headers=headers, tablefmt="github"))

# ‚úÖ Finally, run the function
# load_sheet_data()
# run_mismatch_checker()

def run_semantic_validation():
    import os
    import json
    import time
    import re
    import google.generativeai as genai
    from tqdm import tqdm

    if not os.environ.get("GEMINI_API_KEY"):
       print("‚ö†Ô∏è GEMINI_API_KEY not found in environment ‚Äî semantic validation will be skipped.")
       return

    # os.environ["GEMINI_API_KEY"] = "AIzaSyBaan9lpkE1lVgkNAD0uFSRHWIhKa0GGEc" # üîê Add your actual API key here

    # ‚úÖ Initialize Gemini
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])
    model = genai.GenerativeModel("models/gemini-1.5-pro-latest")

    # ‚úÖ Load mismatches from Agent 2
    with open("/content/mismatch_report.json", "r") as f:
        mismatches = json.load(f)

    # ‚úÖ Load existing match file (may be empty)
    match_file_path = "/content/match_report.json"
    if os.path.exists(match_file_path):
        with open(match_file_path, "r") as f:
            matched = json.load(f)
    else:
        matched = []

    validated = []

    print("üß† Cleaning mismatches using Gemini...\n")

    def normalize_date(text):
        text = text.lower().strip()
        text = re.sub(
            r'\b(january|february|march|april|may|june|july|august|september|october|november|december)\b',
            lambda m: m.group(0)[:3],
            text,
            flags=re.IGNORECASE
        )
        text = text.replace('.', '')
        return text

    def normalize_experience(text):
        return re.sub(r'[^0-9]', '', text)

    # ‚úÖ Track ids in original mismatch
    all_ids_with_mismatches = {m['id'] for m in mismatches}

    for m in tqdm(mismatches):
        field = m.get("field", "").lower()
        sheet_val_raw = str(m.get("sheet_value", "")).strip().lower()
        json_val_raw = str(m.get("json_value", "")).strip().lower()

        # Normalize
        if field == "start_date":
            sheet_val = normalize_date(sheet_val_raw)
            json_val = normalize_date(json_val_raw)
        elif "experience" in field:
            sheet_val = normalize_experience(sheet_val_raw)
            json_val = normalize_experience(json_val_raw)
        else:
            sheet_val = sheet_val_raw
            json_val = json_val_raw

        # Skip if values match after normalization
        if not sheet_val or not json_val or sheet_val == json_val:
            continue

        prompt = (
            "You are an expert in data normalization and semantic validation.\n"
            "Your task is to decide if the following two values represent the *same meaning*, "
            "despite differences in formatting, phrasing, or level of detail.\n\n"
            f"Sheet Value: \"{sheet_val_raw}\"\n"
            f"JSON Value: \"{json_val_raw}\"\n\n"
            "üîç Consider them the SAME if any of the following apply:\n"
            "- üìÖ Dates are the same but written differently (e.g., '12 july 2025' = '12 jul 2025')\n"
            "- They are numerically equivalent but formatted differently (e.g., '3,10,000' = '310000')\n"
            "- Month names and abbreviations must be considered equal (e.g., 'july' = 'jul')\n"
            "- Textual representations of experience match numerically (e.g., '1+ years' = '1')\n"
            "- Sheet value includes extra context but the core value is same (e.g., 'Tools: Mural' = 'mural')\n"
            "- Tools match even if JSON has more tools than Sheet (subset is OK)\n"
            "- Ignore case, punctuation, and minor suffix differences (e.g., 'year' vs 'years')\n\n"
            "- Treat placeholders for 'no data' as equivalent (e.g., 'N.A.', 'NA', 'None', 'null', empty string, or [])\n\n"
            "üö´ Respond ONLY with a single word: 'Yes' if the values mean the same, or 'No' if they are different.\n"
        )

        try:
            response = model.generate_content(prompt)
            decision = response.text.strip().lower()
            if "no" in decision:
                validated.append(m)
        except Exception as e:
            print("‚ö†Ô∏è Error with Gemini:", e)
            validated.append(m)

        time.sleep(1)

    # ‚úÖ Save cleaned mismatches
    with open("/content/validated_mismatches.json", "w") as f:
        json.dump(validated, f, indent=2)

    print(f"\n‚úÖ Cleaned mismatches saved to validated_mismatches.json")
    print(f"‚ùå Reduced from {len(mismatches)} ‚Üí {len(validated)} actual mismatches after semantic validation")

    # ‚úÖ Step: Add to match file any ids which had mismatches earlier but now have none
    still_mismatched_ids = {m['id'] for m in validated}
    resolved_ids = all_ids_with_mismatches - still_mismatched_ids

    # ‚úÖ Avoid duplicates in match report
    existing_matched_ids = {entry['id'] for entry in matched}

    # ‚úÖ Load all ids to validate from ids.json
    with open("/content/all_traversed_ids.json", "r") as f:
        id_entries = json.load(f)  # assumed format: list of { "id": ..., "program_name": ... }
    # Find sheet name and program name for each resolved id
    for resolved_id in resolved_ids:
        entry = next((m for m in id_entries if m['id'] == resolved_id), None)
        if entry and resolved_id not in existing_matched_ids:
            matched.append({
                "id": resolved_id,
                "program_name": entry.get("program_name", "N/A"),
                "sheet": entry.get("sheet", "Unknown")
            })

    # ‚úÖ Save updated match report
    with open(match_file_path, "w") as f:
        json.dump(matched, f, indent=2)

    print(f"üéâ Added {len(resolved_ids)} fully matched programs to match_report.json")


    common_status_report = []

    all_matched_ids = {m['id'] for m in matched}
    all_invalid_ids = {m['id'] for m in validated}  # validated mismatches = actual mismatches after Gemini

    for entry in id_entries:
        program_id = entry.get("id")
        program_name = entry.get("program_name", "N/A")
        if program_id in all_matched_ids:
            status = "valid"
        elif program_id in all_invalid_ids:
            status = "invalid"
        else:
            status = "not found"

        common_status_report.append({
            "id": program_id,
            "program_name": program_name,
            "status": status
        })



    # ‚úÖ Save common status report
    with open("/content/program_status_summary.json", "w") as f:
        json.dump(common_status_report, f, indent=2)

    print("üìÑ Status summary written to program_status_summary.json")


# run_semantic_validation()

import json
import csv
from collections import defaultdict
from IPython.display import Markdown, display
import pandas as pd

def generate_markdown_report():
    # Load mismatches
    with open("/content/validated_mismatches.json", "r") as f:
        mismatches = json.load(f)

    # Group mismatches by sheet name
    grouped_mismatches = defaultdict(list)
    for item in mismatches:
        grouped_mismatches[item["sheet"]].append(item)

    # Load match data
    try:
        with open("/content/match_report.json", "r") as f:
            matches = json.load(f)
    except FileNotFoundError:
        matches = []

    # Load summary data
    try:
        with open("/content/program_status_summary.json", "r") as f:
            summary = json.load(f)
    except FileNotFoundError:
        summary = []

    # Markdown generation (optional)
    lines = ["# ‚ùå Program Mismatch Report\n"]
    for sheet, mismatches in grouped_mismatches.items():
        lines.append(f"## üìÑ Sheet: `{sheet}`\n")
        ids_grouped = defaultdict(list)
        for m in mismatches:
            ids_grouped[m["id"]].append(m)
        for pid, items in ids_grouped.items():
            lines.append(f"### üîπ Program ID: `{pid}`\n")
            lines.append("| Field | Sheet Value | JSON Value |")
            lines.append("|-------|-------------|------------|")
            for item in items:
                field = item["field"]
                sv = str(item["sheet_value"]).replace("\n", " ").strip()
                jv = str(item["json_value"]).replace("\n", " ").strip()
                lines.append(f"| {field} | {sv} | {jv} |")
            lines.append("")
    with open("/content/mismatch_report.md", "w") as f:
        f.write("\n".join(lines))
    print("‚úÖ Markdown report generated at `/content/mismatch_report.md`")

    # Write to Excel
    with pd.ExcelWriter("/content/mismatch_report.xlsx") as writer:
        # 1. Summary Sheet
        if summary:
            pd.DataFrame(summary).to_excel(writer, sheet_name="Status Summary", index=False)

        # 2. Match Sheet
        if matches:
            pd.DataFrame(matches).to_excel(writer, sheet_name="Matches", index=False)

        # 3. Mismatch sheets by sheet name
        for sheet_name, mismatches in grouped_mismatches.items():
            rows = []
            for item in mismatches:
                rows.append({
                    "Program ID": item["id"],
                    "Program Name": item["program_name"],
                    "Field": item["field"],
                    "Sheet Value": str(item["sheet_value"]).replace("\n", " ").strip(),
                    "JSON Value": str(item["json_value"]).replace("\n", " ").strip()
                })
            df = pd.DataFrame(rows)
            # Clean and format sheet name
            final_sheet_name = f"{sheet_name} Mismatches"
            final_sheet_name = final_sheet_name[:31]  # Excel limit
            df.to_excel(writer, sheet_name=final_sheet_name, index=False)

    print("‚úÖ Excel report saved at `/content/mismatch_report.xlsx` with multiple sheets.")

def preview_markdown():
    with open("/content/mismatch_report.md", "r") as f:
        content = f.read()
    display(Markdown(content))


# generate_markdown_report()

# ‚úÖ Imports
import fitz  # PyMuPDF
import requests
import json
import re
import logging
import sys
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from tabulate import tabulate
from difflib import SequenceMatcher
from PIL import Image
import pytesseract
import datetime as dt


# ‚úÖ Logging setup
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

log_file_path = '/content/brochure_mismatch_debug.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file_path),
        logging.StreamHandler(sys.stdout)
    ]
)

# ‚úÖ PDF Extractor
def extract_pdf_pages_from_url(pdf_url, save_path='/tmp/brochure.pdf'):
    logging.info("üóïÔ∏è Downloading brochure PDF from: %s", pdf_url)
    try:
        response = requests.get(pdf_url)
        response.raise_for_status()
        with open(save_path, "wb") as f:
            f.write(response.content)

        doc = fitz.open(save_path)
        return [(i+1, page.get_text().lower()) for i, page in enumerate(doc)]
    except Exception as e:
        logging.error("‚ùå Failed to process brochure: %s", e)
        return []

# ‚úÖ Normalize price to plain number
# def normalize_price(value):
#     value = value.lower().replace(',', '').replace('inr', '').replace('‚Çπ', '').strip()
#     if 'l' in value:
#         match = re.search(r'(\d+(\.\d+)?)\s*l', value)
#         if match:
#             return int(float(match.group(1)) * 100000)
#     elif match := re.search(r'(\d{5,7})', value):
#         return int(match.group(1))
#     return None


MONTHS_MAP = {
    "january": 1, "february": 2, "march": 3, "april": 4,
    "may": 5, "june": 6, "july": 7, "august": 8,
    "september": 9, "october": 10, "november": 11, "december": 12
}

def normalize_price(value):
    value = value.lower().replace(',', '').replace('inr', '').replace('‚Çπ', '').strip()
    if 'l' in value:
        match = re.search(r'(\d+(\.\d+)?)\s*l', value)
        if match:
            return int(float(match.group(1)) * 100000)
    elif match := re.search(r'(\d{5,7})', value):
        return int(match.group(1))
    return None


def normalize_retail_price(value):
    if not value:
        return None
    value = value.lower()
    value = value.replace(',', '')
    value = value.replace('inr', '')
    value = value.replace('‚Çπ', '')
    value = value.replace('+ gst', '')
    value = value.replace('gst extra', '')
    value = value.replace('plus gst', '')
    value = value.strip()

    # Match Lakhs (e.g., 2.2 Lakh, 3.5 lakhs)
    lakh_match = re.search(r'(\d+(\.\d+)?)\s*(lakh|lakhs|l)', value)
    if lakh_match:
        return int(float(lakh_match.group(1)) * 100000)

    # Match raw number (e.g., 220000)
    digit_match = re.search(r'(\d{5,7})', value)
    if digit_match:
        return int(digit_match.group(1))

    return None


# ‚úÖ Fuzzy matcher
def is_fuzzy_match(a, b, threshold=0.8):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= threshold

def get_closest_match(text, options):
    best_score = 0
    best_match = ""
    for option in options:
        score = SequenceMatcher(None, text.lower(), option.lower()).ratio()
        if score > best_score:
            best_score = score
            best_match = option
    return best_score, best_match


# def match_scholarship(val, page_text):
#     val = val.lower().strip()
#     page_text_clean = page_text.lower().replace('\n', ' ').replace('\r', '')

#     # Full month regex
#     MONTH_REGEX = r'(january|february|march|april|may|june|july|august|september|october|november|december)'

#     # ‚úÖ Extract date from sheet value
#     date_match = re.search(rf'(\d{{1,2}})(st|nd|rd|th)?\s+{MONTH_REGEX},?\s*(\d{{4}})?', val)
#     sheet_date = f"{date_match.group(1)} {date_match.group(3)}" if date_match else ""

#     # ‚úÖ Extract amount from sheet value
#     amount_match = re.search(r'(\d{1,3}(,\d{3})+|\d{5,7})', val.replace(',', ''))
#     sheet_amount = normalize_price(amount_match.group(0)) if amount_match else None

#     if not sheet_date or not sheet_amount:
#         return False

#     # ‚úÖ Match date in page text
#     date_found = False
#     for date_text in re.findall(rf'(\d{{1,2}})(st|nd|rd|th)?\s+{MONTH_REGEX}', page_text_clean):
#         brochure_date = f"{date_text[0]} {date_text[2]}"
#         if SequenceMatcher(None, sheet_date, brochure_date).ratio() > 0.85:
#             date_found = True
#             break

#     # ‚úÖ Match amount in page text
#     amount_found = False
#     for amt in re.findall(r'(inr)?\s*([\d.,]+)\s*(l|lakhs|k|thousand)?', page_text_clean):
#         amt_str = ''.join(amt)
#         brochure_amount = normalize_price(amt_str)
#         if brochure_amount == sheet_amount:
#             amount_found = True
#             break

#     return date_found and amount_found


def extract_date_parts(text):
    """
    Extracts (day, month_num, year) from text.
    Month can be full name or short form (Jan, Feb).
    Returns (day, month_num, year) where each is int or None.
    """
    text = text.lower()
    month_regex = r'(january|february|march|april|may|june|july|august|september|october|november|december)'
    match = re.search(rf'(\d{{1,2}})(st|nd|rd|th)?\s+{month_regex},?\s*(\d{{4}})?', text)
    if match:
        day = int(match.group(1))
        month_num = MONTHS_MAP.get(match.group(3).lower())
        year = int(match.group(4)) if match.group(4) else None
        return day, month_num, year
    return None, None, None

def match_scholarship(val, page_text):
    val = val.strip()
    page_text_clean = page_text.lower().replace('\n', ' ').replace('\r', '')

    # Extract date from sheet and brochure
    sheet_day, sheet_month, sheet_year = extract_date_parts(val)
    brochure_dates = re.findall(r'(\d{1,2})(st|nd|rd|th)?\s+(january|february|march|april|may|june|july|august|september|october|november|december),?\s*(\d{4})?', page_text_clean)

    date_found = False
    for d, _, m, y in brochure_dates:
        b_day = int(d)
        b_month = MONTHS_MAP.get(m.lower())
        b_year = int(y) if y else None

        if sheet_day == b_day and sheet_month == b_month:
            date_found = True
            break

    # Extract amount from sheet and brochure
    amount_match = re.search(r'(\d{1,3}(,\d{3})+|\d{5,7})', val.replace(',', ''))
    sheet_amount = normalize_price(amount_match.group(0)) if amount_match else None

    amount_found = False
    for amt in re.findall(r'(inr)?\s*([\d.,]+)\s*(l|lakhs|k|thousand)?', page_text_clean):
        amt_str = ''.join(amt)
        brochure_amount = normalize_price(amt_str)
        if brochure_amount == sheet_amount:
            amount_found = True
            break

    return date_found and amount_found



# ‚úÖ Orientation date matcher
def match_orientation_dates(sheet_val, page_text):
    def extract_dates(text):
        return re.findall(r'(\d{1,2})(st|nd|rd|th)?\s+(january|february|march|april|may|june|july|august|september|october|november|december)(,?\s*\d{4})?', text.lower())

    sheet_dates_raw = extract_dates(sheet_val)
    brochure_dates_raw = extract_dates(page_text)

    sheet_dates = [' '.join([d[0], d[2]]) for d in sheet_dates_raw]
    brochure_dates = [' '.join([d[0], d[2]]) for d in brochure_dates_raw]

    logging.debug("üóìÔ∏è Sheet Dates: %s", sheet_dates)
    logging.debug("üìÑ Brochure Dates: %s", brochure_dates)

    for sheet_d in sheet_dates:
        matched = any(SequenceMatcher(None, sheet_d, brochure_d).ratio() > 0.9 for brochure_d in brochure_dates)
        if not matched:
            return False
    return True

# ‚úÖ Main checker
def check_brochure_mismatches():
    with open('/content/programs.json', 'r') as f:
        programs = json.load(f)
    id_to_brochure_url = {p['id']: p.get('brochure', {}).get('pdf', '') for p in programs if p.get('id')}

    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('/content/service_account.json', scope)
    client = gspread.authorize(creds)
    spreadsheet = client.open_by_url("https://docs.google.com/spreadsheets/d/13s0yN_9fhspaezqqjfZP1OuziIChJM2aD3kmkjP4mhU/edit?gid=353770695#gid=353770695")

    all_mismatches = []
    all_matches = []
    matched_ids_to_sheet = {}
    all_traversed_ids = []
    allowed_fields = [
        'program name', 'duration', 'retail price',
         'class timings',
        'r1 scholarship', 'r2 scholarship', 'batch start date'
    ]

    for sheet in spreadsheet.worksheets():
        sheet_name = sheet.title
        logging.info("üìÑ Processing sheet tab: %s", sheet_name)

        data = sheet.get_all_values()
        if not data or len(data[0]) < 2:
            continue

        headers = data[0]
        row_map = {row[0].strip().lower(): row for row in data[1:] if row[0].strip()}

        for col in range(1, len(headers)):
            program_id = headers[col].strip()
            brochure_url = id_to_brochure_url.get(program_id)
            program_name_row = row_map.get("program name", [""] * len(headers))
            program_name_from_sheet = program_name_row[col].strip() if col < len(program_name_row) else ""

            # Record this traversed ID
            all_traversed_ids.append({
                "id": program_id,
                "sheet": sheet_name,
                "program_name": program_name_from_sheet
            })
            if not brochure_url:
                logging.warning("‚ö†Ô∏è No brochure URL found for ID: %s", program_id)
                continue

            pdf_path = '/tmp/brochure.pdf'
            pages = extract_pdf_pages_from_url(brochure_url, save_path=pdf_path)
            if not pages:
                logging.warning("‚ö†Ô∏è Could not extract brochure for ID: %s", program_id)
                continue

            def search_in_pages(sheet_val, field):
                norm_sheet_val = sheet_val.lower().strip()
                # normalized_sheet_price = normalize_retail_price(norm_sheet_val) if 'price' in field else None

                def clean_text(txt):
                    return re.sub(r'[^a-z0-9 ]', '', txt.lower())

                best_guess = (0, "", 0)

                for pg_num, pg_text in pages:
                    text_cleaned = clean_text(pg_text)
                    val_cleaned = clean_text(norm_sheet_val)

                    if field == 'program name':
                        score = SequenceMatcher(None, val_cleaned, text_cleaned).ratio()
                        if score > best_guess[2]:
                            best_guess = (pg_num, text_cleaned[:100], score)
                        if is_fuzzy_match(val_cleaned, text_cleaned, threshold=0.6):
                            return pg_num, val_cleaned

                    elif field in ['r1 scholarship', 'r2 scholarship','r3 scholarship']:
                        if match_scholarship(norm_sheet_val, pg_text):
                            return pg_num, norm_sheet_val

                    elif field == 'batch start date':
                        if match_orientation_dates(norm_sheet_val, pg_text):
                            return pg_num, norm_sheet_val

                    elif 'price' in field:
                         normalized_sheet_price = normalize_retail_price(norm_sheet_val)

                         if normalized_sheet_price:
                             matches = re.findall(
                                 r'(?:‚Çπ|inr|rs\.?)?\s*'                       # Optional currency symbol
                                 r'([\d,]+(?:\.\d+)?)'                        # Number
                                 r'(?:\s*(lakh|lakhs|l))?'                    # Optional lakh unit
                                 r'(?=\s*(?:\+?\s*gst|gst\b|inclusive of gst)?)',  # GST in non-capturing group
                                 pg_text,
                                 flags=re.IGNORECASE
                             )

                             for m in matches:
                                 num_str, lakh_unit = m
                                 num_str = re.sub(r'[^\d.]', '', num_str)  # remove commas/spaces

                                 if not num_str:
                                     continue  # skip empty matches

                                 if lakh_unit:  # Convert lakh to number
                                     brochure_price = int(float(num_str) * 100000)
                                 else:
                                     brochure_price = int(float(num_str))

                                 logging.info(
                                     f"üîç Comparing brochure price: '{m}' ‚Üí {brochure_price} "
                                     f"with sheet price: {normalized_sheet_price}"
                                 )

                                 if brochure_price == normalized_sheet_price:
                                     return pg_num, f"{num_str}{' lakh' if lakh_unit else ''} + GST"


                    elif norm_sheet_val in pg_text:
                        return pg_num, norm_sheet_val

                    elif field == 'class timings':
                          def extract_slots(text):
                              text = re.sub(r'\s+', ' ', text.lower().replace('*', ''))
                              patterns = re.findall(
                                  r'(?:([a-z]{2,}):)?\s*'  # Optional course code
                                  r'(?:(saturdays?|sundays?|mondays?|tuesdays?|wednesdays?|thursdays?|fridays?)[\s:()-]*)?'  # optional day before
                                  r'0?(\d{1,2})\s*(am|pm)\s*[-‚Äì]\s*0?(\d{1,2})\s*(am|pm)'
                                  r'(?:\s*ist)?(?:\s*on\s*(saturdays?|sundays?|mondays?|tuesdays?|wednesdays?|thursdays?|fridays?))?',  # optional day after
                                  text
                              )

                              normalized_slots = []
                              for match in patterns:
                                  day_raw = match[1] or match[6] or ''
                                  day = day_raw.rstrip('s').capitalize() if day_raw else ''
                                  start_hour, start_meridian = match[2], match[3]
                                  end_hour, end_meridian = match[4], match[5]

                                  try:
                                      start_time = dt.datetime.strptime(f"{int(start_hour)} {start_meridian}", "%I %p")
                                      end_time = dt.datetime.strptime(f"{int(end_hour)} {end_meridian}", "%I %p")
                                  except:
                                      continue

                                  normalized_slots.append((start_time, end_time, day))
                              return normalized_slots

                          sheet_slots = extract_slots(norm_sheet_val)
                          brochure_slots = extract_slots(pg_text)

                          logging.info("üìå Extracted Sheet Slots: %s", [
                              "{} - {} {}".format(s[0].strftime("%I %p"), s[1].strftime("%I %p"), s[2]) for s in sheet_slots
                          ])
                          logging.info("üìÑ Extracted Brochure Slots: %s", [
                              "{} - {} {}".format(b[0].strftime("%I %p"), b[1].strftime("%I %p"), b[2]) for b in brochure_slots
                          ])

                          for s_start, s_end, s_day in sheet_slots:
                              for b_start, b_end, b_day in brochure_slots:
                                  if s_day == b_day and abs((s_start - b_start).total_seconds()) < 60 and abs((s_end - b_end).total_seconds()) < 60:
                                      return pg_num, f"Matched class timing: {s_start.strftime('%I %p')} - {s_end.strftime('%I %p')} {s_day}"


                    elif field == 'tools and frameworks':
                        score = SequenceMatcher(None, val_cleaned, text_cleaned).ratio()
                        if score > best_guess[2]:
                            best_guess = (pg_num, text_cleaned[:100], score)
                        if is_fuzzy_match(val_cleaned, text_cleaned, threshold=0.65):
                            return pg_num, val_cleaned

                # OCR fallback for tools and frameworks
                if field == 'tools and frameworks':
                    try:
                        doc = fitz.open(pdf_path)
                        full_ocr_text = ''
                        for i in range(len(doc)):
                            pix = doc[i].get_pixmap(dpi=200)
                            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                            text = pytesseract.image_to_string(img)
                            full_ocr_text += text.lower() + '\n'
                        tools = [t.strip().lower() for t in re.split(r'[,:\n]', norm_sheet_val) if t.strip()]
                        matched_tools = [t for t in tools if any(t in line for line in full_ocr_text.splitlines())]
                        if matched_tools:
                            return i + 1, f"Tools found via OCR: {', '.join(matched_tools)}"
                    except Exception as e:
                        logging.error("‚ùå OCR failed: %s", e)

                if best_guess[2] > 0.4:
                    return best_guess[0], f"Closest match: {best_guess[1]}"
                return None, None

            for field in allowed_fields:
                if field not in row_map:
                    continue
                row = row_map[field]
                sheet_val = row[col].strip()
                if not sheet_val or sheet_val.lower() in ["na", "n.a.", "n.a", "none"]:
                    continue

                logging.info("üîç Checking '%s': '%s'", field, sheet_val)
                page, snippet = search_in_pages(sheet_val, field)

                if page:
                    logging.info("‚úÖ MATCHED | Found on page %d: %s", page, snippet)
                    # all_matches.append({
                    #       "sheet": sheet_name,
                    #       "id": program_id,
                    #       "field": field,
                    #       "sheet_value": sheet_val,
                    #       "brochure_match_info": snippet
                    #   })
                else:
                    logging.info("‚ùå MISMATCH | '%s' NOT found in brochure.", field)
                    all_mismatches.append({
                        "sheet": sheet_name,
                        "id": program_id,
                        "field": field,
                        "sheet_value": sheet_val,
                        "brochure_match_info": snippet or "NOT FOUND"
                    })


            has_mismatch = any(
                mismatch["id"] == program_id and mismatch["sheet"] == sheet_name
                for mismatch in all_mismatches
            )

            if not has_mismatch:
                matched_ids_to_sheet[program_id] = sheet_name

    with open('/content/brochure_mismatch_report.json', 'w') as f:
        json.dump(all_mismatches, f, indent=2)

         # ‚úÖ Build map of mismatches per program ID
    mismatch_ids = set(item["id"] for item in all_mismatches)

    with open('/content/programs.json', 'r') as f:
         programs = json.load(f)

    match_brochure = []

    for prog in programs:
        prog_id = prog.get("id")
        prog_name = prog.get("program_name")
        sheet_name = matched_ids_to_sheet.get(prog_id)

        if prog_id and sheet_name:  # ‚úÖ Only include programs with successful match
            match_brochure.append({
                "id": prog_id,
                "program_name": prog_name,
                "sheet": sheet_name
            })

    with open('/content/match_brochure.json', 'w') as f:
        json.dump(match_brochure, f, indent=2)

    logging.info("‚úÖ match_brochure.json saved with %d entries.", len(match_brochure))

    # Save mismatch IDs (invalid)
    mismatch_ids = set(item["id"] for item in all_mismatches)

    # Save matched IDs (valid)
    matched_ids = set(item["id"] for item in match_brochure)

    status_summary = []
    seen_ids = set()

    for entry in all_traversed_ids:
        program_id = entry.get("id")
        sheet = entry.get("sheet", "N/A")
        program_name = entry.get("program_name", "N/A")

        seen_ids.add(program_id)

        if program_id in mismatch_ids:
            status = "invalid"
        elif program_id in matched_ids:
            status = "valid"
        else:
            status = "not found"

        status_summary.append({
            "id": program_id,
            "program_name": program_name,
            "sheet": sheet,
            "status": status
        })

    # Add any remaining programs from match_brochure or all_mismatches that were not in all_traversed_ids
    unmatched_ids = (mismatch_ids | matched_ids) - seen_ids
    for program_id in unmatched_ids:
        program_info = next((p for p in programs if p["id"] == program_id), {})
        status_summary.append({
            "id": program_id,
            "program_name": program_info.get("program_name", "N/A"),
            "sheet": matched_ids_to_sheet.get(program_id, "N/A"),
            "status": "not found"
        })

    with open("/content/brochure_status_summary.json", "w") as f:
        json.dump(status_summary, f, indent=2)

    logging.info("üìÑ brochure_status_summary.json saved with %d entries.", len(status_summary))



    print(f"\n‚úÖ Finished. Total mismatches: {len(all_mismatches)}")

    if all_mismatches:
        print(tabulate(all_mismatches, headers="keys", tablefmt="github"))
    else:
        print("üéâ No mismatches found.")

# ‚úÖ Run
# check_brochure_mismatches()

import json
import csv
from collections import defaultdict
from IPython.display import Markdown, display
import pandas as pd

def generate_brochure_markdown_report():
    # ‚úÖ Load mismatches
    with open("/content/brochure_mismatch_report.json", "r") as f:
        mismatches = json.load(f)

    # ‚úÖ Group mismatches by sheet and program id
    grouped = defaultdict(lambda: defaultdict(list))
    for m in mismatches:
        grouped[m["sheet"]][m["id"]].append(m)

    # ‚úÖ Markdown lines
    lines = ["# ‚ùå Brochure Mismatch Report\n"]
    for sheet, programs in grouped.items():
        lines.append(f"## üìÑ Sheet: `{sheet}`\n")
        for pid, items in programs.items():
            lines.append(f"### üîπ Program ID: `{pid}`\n")
            lines.append("| Field | Sheet Value | Brochure Value |")
            lines.append("|-------|-------------|----------------|")
            for item in items:
                field = item["field"]
                sv = str(item["sheet_value"]).replace("\n", " ").strip()
                jv = str(item["brochure_match_info"]).replace("\n", " ").strip()
                lines.append(f"| {field} | {sv} | {jv} |")
            lines.append("")

    # ‚úÖ Save markdown
    with open("/content/brochure_report.md", "w") as f:
        f.write("\n".join(lines))
    print("‚úÖ Markdown saved to `/content/brochure_report.md`")

    # ‚úÖ Collect mismatch rows grouped by sheet
    mismatches_by_sheet = defaultdict(list)
    for sheet, programs in grouped.items():
        for pid, items in programs.items():
            for item in items:
                mismatches_by_sheet[sheet].append({
                    "Sheet": sheet,
                    "Program ID": pid,
                    "Field": item["field"],
                    "Sheet Value": str(item["sheet_value"]).replace("\n", " ").strip(),
                    "Brochure Value": str(item["brochure_match_info"]).replace("\n", " ").strip()
                })

    # ‚úÖ Load matches
    try:
        with open("/content/match_brochure.json", "r") as f:
            matches = json.load(f)
    except FileNotFoundError:
        matches = []

    # ‚úÖ Load summary
    try:
        with open("/content/brochure_status_summary.json", "r") as f:
            summary = json.load(f)
    except FileNotFoundError:
        summary = []

    # ‚úÖ Save all to Excel
    with pd.ExcelWriter("/content/brochure_report.xlsx") as writer:
        # Summary Sheet
        if summary:
            pd.DataFrame(summary).to_excel(writer, sheet_name="Summary", index=False)

        # Matches Sheet
        if matches:
            pd.DataFrame(matches).to_excel(writer, sheet_name="Matches", index=False)

        # Mismatches per sheet
        for sheet, rows in mismatches_by_sheet.items():
            df = pd.DataFrame(rows)
            # df_with_header = pd.DataFrame([["Sheet Name:", sheet]], columns=["Sheet", "Program ID"])
            # df_final = pd.concat([df_with_header, df], ignore_index=True)
            sheet_name_safe = f"{sheet} Mismatches"[:31]  # Excel limit
            df.to_excel(writer, sheet_name=sheet_name_safe, index=False)

    print("‚úÖ Excel saved to `/content/brochure_report.xlsx` with summary, matches, and mismatches by sheet.")

def brochure_preview_markdown():
    with open("/content/brochure_report.md", "r") as f:
        content = f.read()
    display(Markdown(content))


# generate_brochure_markdown_report()
import os

def main(run_semantic=True):
    """
    Non-interactive main entrypoint for CI.
    By default runs both program-page and brochure validations and generates reports.
    Set run_semantic=False to skip the Gemini semantic validation step.
    """
    try:
        # load and run whole pipeline (order same as original '3' option)
        load_sheet_data()
        run_mismatch_checker()
        if run_semantic:
            run_semantic_validation()
            generate_markdown_report()
        else:
            # If semantic is skipped, still try to create markdown from existing files (best-effort)
            try:
                generate_markdown_report()
            except Exception:
                pass

        check_brochure_mismatches()
        try:
            generate_brochure_markdown_report()
        except Exception:
            pass

        print("\n‚úÖ Pipeline finished successfully.")
    except Exception as e:
        import traceback
        print("‚ùå Pipeline failed with exception:", e)
        traceback.print_exc()
        raise

if __name__ == "__main__":
    # Use env var SKIP_SEMANTIC_VALIDATION=true  to skip Gemini step if you don't have a key
    skip_sem = os.environ.get("SKIP_SEMANTIC_VALIDATION", "false").lower() in ("1","true","yes")
    # Pass GEMINI_API_KEY in secrets if you want semantic validation. Make sure to remove any hardcoded API key lines in run_semantic_validation().
    main(run_semantic=not skip_sem)


